---
title: "R Notebook"
output: html_notebook
---

```{r, warning=F, message=F}

rm(list=ls())

require(sf)
require(tidyterra)
require(dismo)
require(tidyverse)
require(terra)
require(predicts)
require(ggnewscale)
require(mgcv)
require(randomForest)
require(maxnet)
require(enmSdmX)
require(gbm)
require(PresenceAbsence)
require(ecospat)
library(pROC)
library(dplyr)
require(caret)

```

# This first code chunk just recreates the maps we built in the lab.

```{r}

# Model building data
vathData = read.csv('https://raw.githubusercontent.com/ValenteJJ/SpatialEcology/main/Week8/vath_2004.csv')

vathPres = vathData %>% filter(VATH==1)
vathAbs = vathData %>% filter(VATH==0)

vathPresXy = as.matrix(vathPres %>% select(EASTING, NORTHING))
vathAbsXy = as.matrix(vathAbs %>% select(EASTING, NORTHING))



# Validation data
vathVal = read.csv('https://raw.githubusercontent.com/ValenteJJ/SpatialEcology/main/Week8/vath_VALIDATION.csv')

vathValPres = vathVal %>% filter(VATH==1)
vathValAbs = vathVal %>% filter(VATH==0)

vathValXy = as.matrix(vathVal %>% select(EASTING, NORTHING))
vathValPresXy = as.matrix(vathValPres %>% select(EASTING, NORTHING))
vathValAbsXy = as.matrix(vathValAbs %>% select(EASTING, NORTHING))



# Bringing in the covariates
elev = rast('https://github.com/ValenteJJ/SpatialEcology/raw/main/Week8/elevation.tif')
canopy = rast('https://github.com/ValenteJJ/SpatialEcology/raw/main/Week8/canopy.tif')
mesic = rast('https://github.com/ValenteJJ/SpatialEcology/raw/main/Week8/mesic.tif')
precip = rast('https://github.com/ValenteJJ/SpatialEcology/raw/main/Week8/precip.tif')


# Resampling to make the covariate rasters match
mesic = resample(x = mesic, y = elev, 'near')
precip = resample(x = precip, y = elev, 'bilinear')

mesic = mask(mesic, elev)
precip = mask(precip, elev)

# Mesic forest within 1 km
probMatrix = focalMat(mesic, 1000, type='circle', fillNA=FALSE)
mesic1km = focal(mesic, probMatrix, fun='sum')


# Building the raster stack
layers = c(canopy, elev, mesic1km, precip)
names(layers) = c('canopy', 'elev', 'mesic1km', 'precip')


#Creating background points
set.seed(23)

backXy = data.frame(backgroundSample(layers, n=2000, p=vathPresXy))

# Extracting covariates for our different points
presCovs = extract(layers, vathPresXy)
absCovs = extract(layers, vathAbsXy)
backCovs = extract(layers, backXy)
valCovs = extract(layers, vathValXy)

presCovs = data.frame(vathPresXy, presCovs, pres=1)
absCovs = data.frame(vathAbsXy, absCovs, pres=0)
backCovs = data.frame(backXy, backCovs, pres=0)
valCovs = data.frame(vathValXy, valCovs)

presCovs = presCovs[complete.cases(presCovs),]
absCovs = absCovs[complete.cases(absCovs),]
backCovs = backCovs[complete.cases(backCovs),]

# Combining presence and background data into one dataframe

backCovs = backCovs %>% select(-ID)
colnames(presCovs)[1:2] = c('x', 'y')
colnames(absCovs)[1:2] = c('x', 'y')

presBackCovs = rbind(presCovs, backCovs)
presAbsCovs = rbind(presCovs, absCovs)

# valCovs = valCovs %>% mutate(VATH = vathVal$VATH)
# valCovs = valCovs[complete.cases(valCovs),]


# Fitting bioclim envelope model
tmp = presCovs %>% select(elev, precip, mesic1km, canopy) %>% 
  as.matrix()

bioclim = envelope(tmp)

bioclimMap = predict(layers, bioclim)



# Fitting GLM
glmModel = glm(pres ~ canopy + elev + I(elev^2) + mesic1km + precip, family='binomial', data=presBackCovs)

glmMap = predict(layers, glmModel, type='response')


# Fitting GAM
gamModel = gam(pres ~ s(canopy, k=6) + s(elev, k=6) + s(mesic1km, k=6) + s(precip, k=6), family='binomial', data=presBackCovs, method='ML')

gamMap = predict(layers, gamModel, type='response')


# Fitting boosted regression tree model

boostModel = gbm(pres ~ elev + canopy + mesic1km + precip, distribution='bernoulli', n.trees=100, interaction.depth=2, shrinkage=0.1, bag.fraction=0.5, data=presBackCovs)

boostMap = predict(layers, boostModel, type='response')
boostMap = mask(boostMap, layers$canopy)


# Fitting random forest model

rfModel = randomForest(as.factor(pres) ~ canopy + elev + mesic1km + precip, data=presBackCovs, mtry=2, ntree=500, na.action = na.omit)

rfMap = predict(layers, rfModel, type='prob', index=2)


#Fitting maxent model

pbVect = presBackCovs$pres
covs = presBackCovs %>% select(canopy:precip)

maxentModel = maxnet(p = pbVect,
                     data= covs,
                     regmult = 1,
                     classes='lqpht')


maxentMap = predictMaxNet(maxentModel, layers, type='logistic')
```



# Challenge 1 (4 points)

In the lab, we fit 6 SDMs. We then calculated discrimination statistics for all 6 and a calibration plot for 1 of them. Create calibration plots for the remaining 5 models, and then make a decision (based on your suite of discrimination statistics and calibration plots) about which of your SDMs is "best." Defend your answer.

```{r}
tmp = valCovs %>% mutate(VATH = vathVal$VATH)
tmp = tmp[complete.cases(tmp),]

valData = data.frame('ID' = 1:nrow(tmp)) %>% 
  mutate(obs = tmp$VATH,
         bioVal = predict(bioclim, tmp %>% select(canopy:precip)),
         glmVal = predict(glmModel, tmp %>% select(canopy:precip), type='response'),
         gamVal = predict(gamModel, tmp %>% select(canopy:precip), type='response'),
         boostVal = predict(boostModel, tmp %>% select(canopy:precip), type='response'),
         rfVal = predict(rfModel, tmp %>% select(canopy:precip), type='prob')[,2],
         maxentVal = predict(maxentModel, tmp %>% select(canopy:precip), type='logistic')[,1])

summaryEval = data.frame(matrix(nrow=0, ncol=9))

nModels = ncol(valData)-2

i = 1

for(i in 1:nModels){
  
  #AUC
  auc = PresenceAbsence::auc(valData, which.model = i)
  
  #Find threshold to maximize Kappa
  kappaOpt = optimal.thresholds(valData, which.model = i, opt.methods=3)
  
  #Sensitivity
  sens = PresenceAbsence::sensitivity(cmx(valData, which.model=i, threshold = kappaOpt[[2]]))
  
  #Specificity
  spec = PresenceAbsence::specificity(cmx(valData, which.model = i, threshold = kappaOpt[[2]]))
  
  #True skill statistic
  tss = sens$sensitivity + spec$specificity - 1
  
  #Kappa
  kappa = Kappa(cmx(valData, which.model = i, threshold = kappaOpt[[2]]))
  
  #Correlation between predicted and realized values
  corr = cor.test(valData[,2], valData[,i+2])$estimate
  
  #Log likelihood
  ll = sum(log(valData[,i+2]*valData[,2] + (1-valData[,i+2]) * (1-valData[,2])))
  ll = ifelse(ll == '-Inf', sum(log(valData[,i+2] + 0.01)*valData[,2] + log((1-valData[,i+2]))*(1-valData[,2])), ll)
  
  #Put them all together and save the values
  summaryI = c(i, auc$AUC, corr, ll, kappaOpt[[2]], sens$sensitivity, spec$specificity, tss, kappa[[1]])
  summaryEval = rbind(summaryEval, summaryI)
}

summaryEval = summaryEval %>% 
  setNames(c('model', 'auc', 'corr', 'll', 'threshold', 'sens', 'spec', 'tss', 'kappa')) %>% 
  mutate(model = colnames(valData)[3:8])

summaryEval

## Calibration plots
calibration.plot(valData, which.model=1, N.bins=20, xlab='predicted', ylab='Observed', main='bioclim')
calibration.plot(valData, which.model=3, N.bins=20, xlab='predicted', ylab='Observed', main='gam')
calibration.plot(valData, which.model=4, N.bins=20, xlab='predicted', ylab='Observed', main='boostModel')
calibration.plot(valData, which.model=5, N.bins=20, xlab='predicted', ylab='Observed', main='rf')
calibration.plot(valData, which.model=6, N.bins=20, xlab='predicted', ylab='Observed', main='maxent')
```

Answer to Challenge 1.

The GLM model (glmVal) has the highest AUC (0.673), indicating it has the best overall discriminatory power among the models tested. The Boosted model (boostVal) shows the highest sensitivity (0.631), indicating it's the best at identifying true presences. The MaxEnt model (maxentVal) has the highest specificity (0.799), making it the best at identifying true absences. The GLM model (glmVal) has the highest TSS (0.274), suggesting a good balance in its predictive performance. The MaxEnt model (maxentVal) and GLM model (glmVal) are close, with MaxEnt slightly leading (0.140 vs. 0.137). Both indicate a fair level of agreement beyond chance, with MaxEnt slightly ahead.
Given the metrics, the GLM model (glmVal) appears to be the best model overall, particularly due to its highest AUC and TSS values. These metrics suggest that it not only has superior discriminatory power but also maintains a good balance between sensitivity and specificity, making it effective for both identifying presences and avoiding false positives.



# Challenge 2 (4 points)

Each SDM we created uses a different algorithm with different assumptions. Because of this, ecologists frequently use "ensemble" approaches that aggregate predictions from multiple models in some way. Here we are going to create an ensemble model by calculating a weighted average of the predicted occupancy values at each pixel. We will calculate weights based on model AUC values to ensure that the models with the best AUC values have the most influence on the predicted values in the ensemble model.

Create a raster stack that combines the glmMap, gamMap, boostMap, and rfMap (hint use c()).

Next, create a vector of the AUC values for each model.

Lastly, use the weighted.mean() function in the terra package to create the new raster as a weighted average of the previous 4 rasters.

Plot the result, and explain why we left out the bioclim and Maxent models for this ensemble model.

```{r}
raster_stack <- rast(c(glmMap, gamMap, boostMap, rfMap))
print(raster_stack)

auc_values <- c(glmAUC = 0.599, gamAUC = 0.598, boostAUC = 0.622, rfAUC = 0.591)
auc_values

auc_weights <- auc_values / sum(auc_values)
auc_weights

weighted_average <- (glmMap * auc_weights[1] + gamMap * auc_weights[2] + 
                     boostMap * auc_weights[3] + rfMap * auc_weights[4])

print(weighted_average)
plot(weighted_average, main="Weighted Average Ensemble Prediction")
```

Answer to Challenge 2.

For an ensemble model, it's often desirable to combine models that have similar assumptions, data requirements, and output types for methodological consistency. This ensures the ensemble prediction is robust and interpretable. Bioclim provides a suitability index rather than a direct probability of presence, which might not combine well with other models' probabilistic outputs. Maxent, while it does produce probabilities, might do so in a way that doesn't align perfectly with the logistic regression-based outputs of GLM, GAM, RF, and Boosted models.
Both Bioclim and Maxent can produce more complex model outputs that might not align well with simpler models in an ensemble context. Ensuring interpretability and consistency across models could potentially be a reason for their exclusion.




# Challenge 3 (4 points)

Is this ensemble model an improvement over one of the models you built previously? Provide evidence and explain the criteria you used to come to your conclusion.

```{r}
coords <- cbind(valCovs$EASTING, valCovs$NORTHING)


# Extract ensemble model predictions for validation points
ensemble_preds <- extract(weighted_average, coords)

valid_indices <- complete.cases(valData$obs, ensemble_preds[, 1])

# Apply valid_indices to subset both observed statuses and predictions.
observed_status_no_na <- valData$obs[valid_indices]
ensemble_preds_no_na <- ensemble_preds[valid_indices, 1] 

print(length(observed_status_no_na))
print(length(ensemble_preds_no_na))

observed_status_no_na <- factor(observed_status_no_na, levels = c(0, 1))
library(pROC)
roc_result <- roc(observed_status_no_na, ensemble_preds_no_na)
auc_value <- auc(roc_result)
auc_value

plot(roc_result, main="ROC Curve for Ensemble Model")
abline(a=0, b=1, lty=2, col="red")
```
Answer to Challenge 3.

I used the AUC (Area Under the ROC Curve) metric as the primary metric for comparison between the ensemble model and the other models. AUC values range from 0 to 1, where a higher value indicates better model performance. An AUC value closer to 1 suggests that the model has a good measure of separability; it can distinguish between positive and negative classes effectively.
The ensemble model, with an AUC of 0.6712, performs better than the BioClim, GAM, Boosted Model, Random Forest, and MaxEnt models. This indicates that combining some of these models into an ensemble has resulted in a model that, overall, is more capable of distinguishing between the presence and absence classes than most individual models.
Comparison with GLM: The ensemble model's performance is very close to that of the GLM model, which has an AUC of 0.673. The difference here is minimal, suggesting that the ensemble model and GLM have nearly equivalent discriminative abilities.
To conclude, the ensemble model does show an improvement over most of the individual models and is nearly equivalent to the best-performing individual model (GLM). Whether or not to prefer the ensemble model over the GLM should be based on considerations beyond just AUC, including model complexity, interpretability, computational resources, and specific application needs.


# Challenge 4 (4 points)

In the lab we built models using presence-background data then validated those models with presence-absence data. For this challenge, you're going to compare the predictive ability of a model built using presence-background data with one built using presence-absence data.

Fit a GLM using the presence-background data as we did in the lab (i.e., use the presBackCovs dataframe). Fit a second GLM using the presence-absence data (i.e., use the presAbsCovs dataframe). Validate both of these models on the novel presence-absence data (valCovs dataset). Specifically, calculate and compare AUC, Kappa, and TSS for these two models. Which model does a better job of prediction for the validation data and why do you think that is? 

```{r}
# Fitting GLM using presBackCovs
glmModel1 = glm(pres ~ canopy + elev + I(elev^2) + mesic1km + precip, family='binomial', data=presBackCovs)
predictions_glm1 <- predict(glmModel1, newdata=valCovs, type="response")

# Fitting GLM using presAbsCovs
glmModel2 = glm(pres ~ canopy + elev + I(elev^2) + mesic1km + precip, family='binomial', data=presAbsCovs)
predictions_glm2 <- predict(glmModel2, newdata=valCovs, type="response")

# AUC for glmModel1
roc_glm1 <- roc(response=valCovs$VATH, predictor=predictions_glm1)
auc_glm1 <- auc(roc_glm1)
cat("AUC for GLM Model 1: ", auc_glm1, "\n")

predictions_binary_glm1 <- ifelse(predictions_glm1 > 0.5, 1, 0)
conf_mat_glm1 <- confusionMatrix(factor(predictions_binary_glm1), factor(valCovs$VATH))
kappa_glm1 <- conf_mat_glm1$overall["Kappa"]
sensitivity_glm1 <- conf_mat_glm1$byClass["Sensitivity"]
specificity_glm1 <- conf_mat_glm1$byClass["Specificity"]
tss_glm1 <- sensitivity_glm1 + specificity_glm1 - 1
cat("Kappa for GLM Model 1: ", kappa_glm1, " TSS for GLM Model 1: ", tss_glm1, "\n")

# AUC for glmModel2
roc_glm2 <- roc(response=valCovs$VATH, predictor=predictions_glm2)
auc_glm2 <- auc(roc_glm2)
cat("AUC for GLM Model 2: ", auc_glm2, "\n")

predictions_binary_glm2 <- ifelse(predictions_glm2 > 0.5, 1, 0)
conf_mat_glm2 <- confusionMatrix(factor(predictions_binary_glm2), factor(valCovs$VATH))
kappa_glm2 <- conf_mat_glm2$overall["Kappa"]
sensitivity_glm2 <- conf_mat_glm2$byClass["Sensitivity"]
specificity_glm2 <- conf_mat_glm2$byClass["Specificity"]
tss_glm2 <- sensitivity_glm2 + specificity_glm2 - 1
cat("Kappa for GLM Model 2: ", kappa_glm2, " TSS for GLM Model 2: ", tss_glm2, "\n")
```

Answer to Challenge 4.

Based on the results, GLM Model 2, which was trained using presence-absence data, performs better on the validation dataset compared to GLM Model 1, which was trained using presence-background data. 
AUC (Area Under the ROC Curve): GLM Model 2 has a higher AUC value (0.7240648) compared to GLM Model 1 (0.6726221). This indicates that Model 2 is better at distinguishing between presence and absence of varied thrush, as a higher AUC value signifies a model's better capability to discriminate between positive and negative classes.
Kappa Statistic: GLM Model 2 has a Kappa value of 0.05884853, while GLM Model 1 has a Kappa value of 0. This means Model 2 has a slight improvement in agreement between observed and predicted classifications over chance agreement, although the value is still close to 0, indicating a weak agreement beyond chance.
TSS (True Skill Statistic): GLM Model 2 also has a higher TSS value (0.03825388) compared to GLM Model 1 (0). TSS combines sensitivity and specificity into a single metric that ranges from -1 to 1, where 1 indicates perfect performance and 0 indicates no performance beyond random chance. A positive TSS value for Model 2 indicates it has some ability to balance sensitivity and specificity better than Model 1, which has a TSS of 0.
Overall, GLM Model 2's better performance could be due to the nature of the training data. Presence-absence data provides direct information on where the species is both present and absent, which may offer a more comprehensive understanding for the model to learn from compared to presence-background data, where absences are not confirmed but assumed based on background data points. This can lead to a clearer distinction between presence and absence conditions in the model, as reflected in the improved AUC, Kappa, and TSS metrics for Model 2.


# Challenge 5 (4 points)

Now calculate the same statistics (AUC, Kappa, and TSS) for each model you developed in Challenge 4 using K-fold validation with 5 groups. Do these models perform better or worse based on K-fold validation (as compared to validation based on novel data)? Why might that occur?

```{r}
datasets <- list(presBackCovs, presAbsCovs)
names(datasets) <- c("Model1", "Model2")

results <- list()

# K-Fold
set.seed(123) # For reproducibility
folds <- createFolds(presBackCovs$pres, k = 5, list = TRUE, returnTrain = TRUE)

for(model_name in names(datasets)) {
  dataset <- datasets[[model_name]]
  auc_values <- numeric(length(folds))
  kappa_values <- numeric(length(folds))
  tss_values <- numeric(length(folds))
  
  for(i in seq_along(folds)) {
    train_indices <- folds[[i]]
    test_indices <- setdiff(1:nrow(dataset), train_indices)
    
    train_data <- dataset[train_indices, ]
    test_data <- dataset[test_indices, ]
    
    # Fit model
    glm_model <- glm(pres ~ canopy + elev + I(elev^2) + mesic1km + precip, family = "binomial", data = train_data)
    
    # Predictions
    predictions <- predict(glm_model, newdata = test_data, type = "response")
    
    # Calculate metrics
    roc_res <- roc(test_data$pres, predictions)
    auc_values[i] <- auc(roc_res)
    
    # Converting probabilities to binary predictions based on a threshold
    predicted_class <- ifelse(predictions > 0.5, 1, 0)
    cm <- confusionMatrix(factor(predicted_class), factor(test_data$pres))
    
    kappa_values[i] <- cm$overall['Kappa']
    sensitivity <- cm$byClass['Sensitivity']
    specificity <- cm$byClass['Specificity']
    tss_values[i] <- sensitivity + specificity - 1
  }
  
  # Storing results
  results[[model_name]] <- list(
    AUC = mean(auc_values),
    Kappa = mean(kappa_values),
    TSS = mean(tss_values)
  )
}

# Print results
print(results)
```

Answer to Challenge 5.

The models perform better based on K-fold validation compared to validation on novel data. Both models show relatively high AUC values, with Model 1 slightly outperforming Model 2. This suggests that both models have a good measure of separability and are capable of distinguishing between presence and absence (or background) reasonably well. The AUC values here are higher than those obtained from validation on novel data, indicating that the models may perform better on data similar to what they were trained on, as captured by the cross-validation process.
SImilar to using the novel data, the Kappa statistic for Model 1 is 0, indicating no agreement between the observed and predicted classifications, beyond what would be expected by chance. Model 2 has a Kappa of 0.07550204 suggesting a slight agreement beyond chance.
And similar to Kappa, a TSS of 0 for Model 1 indicates no skill in prediction over random chance, while Model 2 shows a small positive value (0.04747896), indicating some skill over random prediction.

The better performance in K-fold cross-validation compared to validation on novel data suggests that these models are well-tuned to the characteristics of the data they were trained on, and their ability to generalize might be more accurately reflected in a cross-validation setting. It also underscores the potential variability and challenge in predicting novel data that may have different characteristics or distributions not captured during training.